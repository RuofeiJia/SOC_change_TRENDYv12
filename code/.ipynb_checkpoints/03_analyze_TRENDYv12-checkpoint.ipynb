{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for analysis of the soil carbon sink in TRENDYv12 S2 simulations (masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data should be downloaded using 01_download_data_and_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate surface area of each pixel\n",
    "def calc_pixel_area(raster:xr.DataArray) -> xr.DataArray:\n",
    "    '''\n",
    "    Calculate the area of each pixel in a raster\n",
    "\n",
    "    Parameters:\n",
    "    raster (xarray.DataArray): raster to calculate pixel area for\n",
    "\n",
    "    Returns:\n",
    "    xarray.DataArray: raster with pixel area as values\n",
    "    '''\n",
    "\n",
    "    # get the resolution of the raster\n",
    "    res = raster.rio.resolution()\n",
    "\n",
    "    l1 = np.radians(raster['y']- np.abs(res[1])/2)\n",
    "    l2 = np.radians(raster['y']+ np.abs(res[1])/2)\n",
    "    dx = np.radians(np.abs(res[0]))    \n",
    "    _R = 6371e3  # Radius of earth in m. Use 3956e3 for miles\n",
    "\n",
    "    # calculate the area of each pixel\n",
    "    area = _R**2 * dx * (np.sin(l2) - np.sin(l1))\n",
    "\n",
    "    # create a new xarray with the pixel area as values\n",
    "    result = ((raster-raster+1)*area)\n",
    "\n",
    "    # set the nodata value    \n",
    "    if raster.rio.nodata is None:\n",
    "        result.rio.set_nodata(np.nan,inplace=True)\n",
    "    else:\n",
    "        result.rio.set_nodata(raster.rio.nodata,inplace=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(model:str) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Get the area of each pixel for a given model\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray\n",
    "        the area of each pixel\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'CLM5.0':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area_ds = xr.open_dataset(f'{data_dir}/cell_area/CLM5.0_S2_area.nc')\n",
    "        area = area_ds['area']*area_ds['landfrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model == 'DLEM':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/DLEM_land_area.nc')['LAND_AREA']*1e6\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model in ['IBIS','OCN','ORCHIDEE','LPJml']:\n",
    "        if model == \"LPJml\":\n",
    "            model = \"LPJmL\"\n",
    "        # load the ocean cover fraction data\n",
    "        ocean = xr.open_dataset(f'{data_dir}/cell_area/{model}_S2_oceanCoverFrac.nc',decode_times=False)['oceanCoverFrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        ocean = ocean.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        \n",
    "        # the land data is the cell area times the fraction of the cell that is not ocean\n",
    "        area = calc_pixel_area(ocean)*(1-ocean)\n",
    "    elif model == 'CLASSIC':\n",
    "        # load land fraction data\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/CLASSIC_S2_land_fraction.nc')['sftlf'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "    elif model == 'ED':\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/EDv3_landCoverFrac.nc')['landArea'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "    elif model == 'ELM':\n",
    "\n",
    "        # load the a file with the base resolution to reproject the area onto\n",
    "        ds = xr.open_dataset(glob(f'../data/{\"ELM\"}/S2/*{\"nbp\"}*.nc')[0],decode_times=False)\n",
    "\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "\n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "        ds = ds.swap_dims({'lon':'x','lat':'y'})\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "        ds.rio.write_crs('EPSG:4326',inplace=True)\n",
    "        ds = ds['nbp'][0,:,:]\n",
    "\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/areacella_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['areacella'].rename({'lat': 'y', 'lon': 'x'})\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/sftlf_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['sftlf'].rename({'lat': 'y', 'lon': 'x'})/100\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "\n",
    "        # change the coordinates to start from -180 to 180\n",
    "        area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "        area = area.sortby(['x','y'])\n",
    "        \n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "\n",
    "        # reproject the area into the base resolution\n",
    "        area = area.rio.write_crs('EPSG:4326',inplace=True).rio.reproject_match(ds,resampling=Resampling.sum)\n",
    "        area = area.where(area<1e30)\n",
    "\n",
    "    elif model == 'ISBACTRIP':\n",
    "        # load the grid cell area file and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_area.nc')['AREA'].rename({'LAT_FULL':'y','LON_FULL':'x'})\n",
    "        \n",
    "        # load the land area fraction data and rename coordinates\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_S2_sftlf.nc',decode_times=False)['sftlf'].mean(dim='time_counter').rename({'lat_FULL':'y','lon_FULL':'x'})\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "    elif model == 'JULES':\n",
    "        \n",
    "        # load the lancdAreaFrac from trendy-v10\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc')['landFrac']\n",
    "        \n",
    "        # renanme the coordinates\n",
    "        land_fraction = land_fraction.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "        # the land area is the grid cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "\n",
    "    return area\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fraction(model:str) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Get predicted fraction (area extent) used to estimate SOC change\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray\n",
    "        the predicted fractions of each pixel of this model\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the unique resolution index for model\n",
    "    models_res1 = ['CLASSIC']\n",
    "    models_res2 = ['DLEM']\n",
    "    models_res3 = ['ED']\n",
    "    models_res4 = ['ELM']\n",
    "    models_res5 = ['IBIS', 'ISAM', 'LPJ-GUESS', 'LPJml', 'LPJwsl', 'lpxqs', 'ORCHIDEE', 'VISIT']\n",
    "    models_res6 = ['ISBACTRIP']\n",
    "    models_res7 = ['JSBACH']\n",
    "    models_res8 = ['JULES']\n",
    "    models_res9 = ['OCN', 'SDGVM']\n",
    "    models_res10 = ['YIBS']\n",
    "\n",
    "    if model in models_res1:\n",
    "        res_index = 1\n",
    "    elif model in models_res2:\n",
    "        res_index = 2\n",
    "    elif model in models_res3:\n",
    "        res_index = 3\n",
    "    elif model in models_res4:\n",
    "        res_index = 4\n",
    "    elif model in models_res5:\n",
    "        res_index = 5\n",
    "    elif model in models_res6:\n",
    "        res_index = 6\n",
    "    elif model in models_res7:\n",
    "        res_index = 7\n",
    "    elif model in models_res8:\n",
    "        res_index = 8\n",
    "    elif model in models_res9:\n",
    "        res_index = 9\n",
    "    elif model in models_res10:\n",
    "        res_index = 10\n",
    "\n",
    "    # open fraction file\n",
    "    frac_dir = f\"{data_dir}/prediction_fractions/fraction_resolution{res_index}.nc\"\n",
    "    frac_var = f\"fraction_resolution{res_index}\"\n",
    "    frac = xr.open_dataset(frac_dir)[frac_var]\n",
    "\n",
    "    # format file\n",
    "    frac = frac.rename('fraction')\n",
    "    if res_index in [1,4,7,10]:\n",
    "        frac = frac.rename({'northing': 'y', 'easting': 'x'})\n",
    "    elif res_index in [2,3,5,6,8,9]:\n",
    "        frac = frac.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    return frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model(model:str,var:str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Parse the data for a given model and variable\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "    var: str\n",
    "        the name of the variable\n",
    "\n",
    "    Returns:\n",
    "    xr.Dataset\n",
    "        the parsed data\n",
    "    \"\"\"\n",
    "\n",
    "    # open the dataset\n",
    "    ds = xr.open_dataset(glob(f'../data/{model}/S2/*{var}*.nc')[0],decode_times=False)\n",
    "\n",
    "    # convert coordinates to standard time,y,x\n",
    "    if 'time' not in ds.sizes.keys():\n",
    "       ds = ds.rename({'time_counter':'time'}) \n",
    "    if 'lon' in ds.dims:\n",
    "        ds = ds.rename({'lon':'x','lat':'y'})\n",
    "    elif 'longitude' in ds.dims:\n",
    "        ds = ds.rename({'longitude':'x','latitude':'y'})\n",
    "    else:\n",
    "        ds = ds.rename({'lon_FULL':'x','lat_FULL':'y'})\n",
    "    \n",
    "    # set the time coordinate to datetime based on the size of the file\n",
    "    if ds.sizes['time'] == 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1860', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] > 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] >300:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='YS')\n",
    "    else:\n",
    "        ds['time'] = pd.date_range(start='01-01-2002', periods=len(ds.time), freq='MS')\n",
    "    \n",
    "    if 'nbnd' in ds.dims:\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "        \n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "    if 'bnds' in ds.dims:\n",
    "        # if the dataset had a bnds coordinate drop it\n",
    "        ds = ds.drop_dims('bnds')\n",
    "\n",
    "    # order the coordinates\n",
    "    ds = ds.transpose('time','y','x')\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "\n",
    "    # if the data is in the 0-360 range, convert it to -180-180\n",
    "    if ds['x'].min()>=0:\n",
    "        ds.coords['x'] = xr.where(ds.coords['x']>=180, ds.coords['x']-360, ds.coords['x'])\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "    \n",
    "    # if the variable is not in the data_vars, rename it\n",
    "    ds_var = list(ds.data_vars.keys())[0]\n",
    "    if var not in ds.data_vars:\n",
    "        ds = ds.rename({ds_var:var})\n",
    "\n",
    "    # # get the land area of each pixel\n",
    "\n",
    "    # # define the models that need special attention\n",
    "    models_to_fix = ['CLM5.0','DLEM','IBIS','OCN','ORCHIDEE','ISBACTRIP','JULES','CLASSIC','ED','ELM','LPJml']\n",
    "\n",
    "    # if the model needs special attention, use the get_area function to calculate the land area\n",
    "    if model in models_to_fix:\n",
    "        area = get_area(model)\n",
    "        if 'time' in area.dims:\n",
    "            area = area.sel(time=area['time'][0]).drop_vars('time')\n",
    "        if area['x'].min()>=0:\n",
    "            area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "            # sort the data based on y and x\n",
    "            area = area.sortby(['y','x'])\n",
    "    else:\n",
    "\n",
    "        # otherwise use the calc_pixel_area function to calculate the land area\n",
    "        area = calc_pixel_area(ds[var][1,:,:])\n",
    "\n",
    "    # name the land_area DataArray\n",
    "    area.name = 'land_area'\n",
    "\n",
    "    # take the annual average of the data\n",
    "    ds = ds.resample(time='YS').mean()\n",
    "    \n",
    "    if model in ['CLASSIC','CLM5.0']:\n",
    "        # if the model is CLASSIC or CLM5.0, shift years by one\n",
    "        ds['time'] = (ds['time'].to_series() + pd.DateOffset(years=1)).values\n",
    "\n",
    "    # return a merged dataset of the data and the land area\n",
    "    # result = xr.merge([ds,area])\n",
    "\n",
    "    # # get predicted fraction for SOC change estimations\n",
    "    frac = get_fraction(model) \n",
    "\n",
    "    # if resampled fraction and area have same dimension, use area x,y values on frac to ensure matching\n",
    "    area = area.sortby(['y','x'])\n",
    "    frac = frac.sortby(['y','x'])\n",
    "    \n",
    "    if area.dims == frac.dims and area.shape == frac.shape:\n",
    "        frac = frac.assign_coords(y=area.y, x=area.x)\n",
    "    else: \n",
    "        print(model)\n",
    "        print(\"area and frac have different dimensions or shapes\")\n",
    "        print(f\"area {area.shape}\")\n",
    "        print(f\"frac {frac.shape}\")\n",
    "    \n",
    "    # return result\n",
    "    return ds,area,frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "must supply at least one object to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/utils.py\u001b[0m in \u001b[0;36mpeek_at\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d5b4f81c542d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# merge the datasets along the pool dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel_merged_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# extract the model name from the file name and not the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must supply at least one object to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_VALID_COMPAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: must supply at least one object to concatenate"
     ]
    }
   ],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_nbp = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "    \n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "    \n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "\n",
    "        # if the variable is a nbp variable, parse the model\n",
    "        if var in ['nbp','nbpAnnual']:\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar = parse_model(model,var)\n",
    "\n",
    "            # add the product of the nbp and surface area to get units of KgC s-1 per gridcell\n",
    "            model_dss.append(dss[var]*ar)\n",
    "    \n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('/')[-1].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "\n",
    "    # calculate the global nbp - convert from kgC s-1 to PgC yr-1\n",
    "    global_nbp = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15 * 365*24*3600\n",
    "    global_nbp.name = model2\n",
    "    \n",
    "    # append the global nbp to the list\n",
    "    parsed_global_nbp.append(global_nbp)\n",
    "\n",
    "# merge the global nbp datasets along the model dimension\n",
    "models_global_nbp = xr.concat(parsed_global_nbp,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_nbp['model'] = [x.name for x in parsed_global_nbp]\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_nbp.name = 'nbp'\n",
    "models_global_nbp_df = models_global_nbp.to_dataframe()['nbp'].unstack()\n",
    "models_global_nbp_df.columns = models_global_nbp_df.columns.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas requires version '3.0.0' or newer of 'openpyxl' (version '2.5.6' currently installed).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2b0fd52e033e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the GCB2023 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mGCB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://globalcarbonbudgetdata.org/downloads/archive/Global_Carbon_Budget_2023v1.1.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Terrestrial Sink'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mGCB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mGCB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfsspec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mappropriate\u001b[0m \u001b[0mURLs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_get_filepath_or_buffer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \"\"\"\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas requires version '3.0.0' or newer of 'openpyxl' (version '2.5.6' currently installed)."
     ]
    }
   ],
   "source": [
    "# load the GCB2023 data\n",
    "GCB = pd.read_excel('https://globalcarbonbudgetdata.org/downloads/archive/Global_Carbon_Budget_2023v1.1.xlsx',sheet_name='Terrestrial Sink',skiprows=27)\n",
    "GCB.set_index('Year',inplace=True)\n",
    "GCB = GCB.iloc[:,2:-3]\n",
    "\n",
    "# change the model names to match our analysis\n",
    "models_GCB = list(GCB.columns)\n",
    "models_GCB[4] = 'EDv3'\n",
    "models_GCB[5] = 'E3SM'\n",
    "models_GCB[10] = 'JULES'\n",
    "models_GCB[11] = 'LPJ-GUESS'\n",
    "models_GCB[13] = 'LPJmL'\n",
    "models_GCB[15] = 'OCN'\n",
    "models_GCB[16] = 'ORCHIDEE'\n",
    "GCB.columns = models_GCB\n",
    "\n",
    "# assert that the RMSE for all models is less than 7%\n",
    "assert all((((models_global_nbp_df.T.loc[1959:2022] - GCB.loc[1959:2022])**2).mean()**0.5/GCB.loc[1959:2022].mean()*100).dropna().round(2).values < 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis for soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/CLASSIC/S2',\n",
       " '../data/DLEM/S2',\n",
       " '../data/ED/S2',\n",
       " '../data/ELM/S2',\n",
       " '../data/IBIS/S2',\n",
       " '../data/ISAM/S2',\n",
       " '../data/ISBACTRIP/S2',\n",
       " '../data/JSBACH/S2',\n",
       " '../data/JULES/S2',\n",
       " '../data/LPJ-GUESS/S2',\n",
       " '../data/LPJml/S2',\n",
       " '../data/LPJwsl/S2',\n",
       " '../data/OCN/S2',\n",
       " '../data/ORCHIDEE/S2',\n",
       " '../data/SDGVM/S2',\n",
       " '../data/VISIT/S2',\n",
       " '../data/YIBS/S2',\n",
       " '../data/lpxqs/S2']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "dirs = [d for d in dirs if d not in [\"../data/CABLEPOP/S2\", \"../data/CLM5.0/S2\"]]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:01<00:25,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIC multiplied var shape(322, 180, 360)\n",
      "DLEM multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [00:06<00:55,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ED multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [00:12<00:42,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELM multiplied var shape(323, 192, 288)\n",
      "IBIS multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [01:21<05:52, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISAM multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [01:56<03:46, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISBACTRIP multiplied var shape(324, 150, 360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [01:57<02:21, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSBACH multiplied var shape(323, 96, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3671197/2818166508.py:96: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  land_fraction = xr.open_dataset(f'{data_dir}/cell_area/JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc')['landFrac']\n",
      " 50%|█████     | 9/18 [01:57<01:28,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULES multiplied var shape(323, 144, 192)\n",
      "LPJ-GUESS multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [02:00<01:02,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPJml multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [02:03<00:44,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPJwsl multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [02:07<00:19,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCN multiplied var shape(323, 180, 360)\n",
      "ORCHIDEE multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [02:11<00:08,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDGVM multiplied var shape(323, 180, 360)\n",
      "VISIT multiplied var shape(163, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [02:19<00:03,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YIBS multiplied var shape(323, 181, 360)\n",
      "lpxqs multiplied var shape(323, 360, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [03:31<00:00, 11.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "# based on O'Sullivan 2022 et al. for CLM5 cLitter is included in cSoil and for CABLEPOP cCwd is included in cLitter\n",
    "# exclude 'CLM5.0' and 'CABLEPOP\n",
    "dirs = [d for d in dirs if d not in [\"../data/CABLEPOP/S2\", \"../data/CLM5.0/S2\"]]\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_cSoil = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "\n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "\n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        if var in ['cSoil']: # exclude 'cCwd','cLitter'\n",
    "            \n",
    "            # # based on O'Sullivan 2022 et al. for CLM5 cLitter is included in cSoil and for CABLEPOP cCwd is included in cLitter\n",
    "            # # exclude 'CLM5.0' and 'CABLEPOP'\n",
    "            # if (model == 'CLM5.0') or (model == 'CABLEPOP'):\n",
    "            #     continue\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar,frac = parse_model(model,var)\n",
    "\n",
    "            # add the product of cSoil, surface area and predicted area fraction to get units of KgC per gridcell\n",
    "            model_dss.append(dss[var]*ar*frac)\n",
    "            var_multiplied = dss[var]*ar*frac\n",
    "            print(f\"{model} multiplied var shape{var_multiplied.shape}\")\n",
    "\n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "    \n",
    "    # calculate the global stocks - convert from kgC to PgC\n",
    "    global_cSoil = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15\n",
    "    global_cSoil.name = model2\n",
    "    \n",
    "    # append the global stocks to the list\n",
    "    parsed_global_cSoil.append(global_cSoil)\n",
    "\n",
    "# merge the global stocks datasets along the model dimension\n",
    "models_global_cSoil = xr.concat(parsed_global_cSoil,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_cSoil['model'] = [x.name for x in parsed_global_cSoil]\n",
    "models_global_cSoil.name = 'cSoil'\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_cSoil_df = models_global_cSoil.to_dataframe()['cSoil'].unstack()\n",
    "models_global_cSoil_df.columns = models_global_cSoil_df.columns.year\n",
    "\n",
    "# save the dataframes to csv\n",
    "models_global_cSoil_df.to_csv('../results/TRENDY_v12_global_cSoil_S2_fraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked with same area as data-driven estimation, he average SOC stock change from 1992 to 2022 for the TRENDY v12 models is 0.24 PgC yr-1\n"
     ]
    }
   ],
   "source": [
    "SOC_stock_change = models_global_cSoil_df.diff(axis=1).loc[:,1992:2022].mean(axis=1).mean()\n",
    "\n",
    "print(f'Masked with same area as data-driven estimation, he average SOC stock change from 1992 to 2022 for the TRENDY v12 models is {SOC_stock_change:.2f} PgC yr-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

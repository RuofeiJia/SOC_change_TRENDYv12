{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for analysis of the soil carbon sink in TRENDYv12 S2 simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relevant files from Mike O'Sullivan's gihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/1h3dk6395tgd9bb5wqtmc9m00000gn/T/ipykernel_7405/979931139.py:4: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  TRENDY_FILES = pd.read_json(file_index.read().decode('utf-8'))\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "url = 'https://raw.githubusercontent.com/mdosullivan/GCB/main/fileIndex.json'\n",
    "file_index = urlopen(url)\n",
    "TRENDY_FILES = pd.read_json(file_index.read().decode('utf-8'))\n",
    "\n",
    "# Take only the TRENDY v12 files from the S2 simulation, and only the carbon pools. Exclude CARDAMOM.\n",
    "TRENDY_v12 = TRENDY_FILES[TRENDY_FILES[0].str.contains('trendyv12')]\n",
    "TRENDY_v12_S2 = TRENDY_v12[TRENDY_v12[0].str.contains('/S2/')]\n",
    "TRENDY_v12_S2_cPools = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('_cSoil\\.|cVeg\\.|cLitter\\.|cCwd\\.|cProduct\\.')]\n",
    "TRENDY_v12_S2_cPools = TRENDY_v12_S2_cPools[~TRENDY_v12_S2_cPools[0].str.contains('CARDAMOM')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRENDY_v12_S2_nbp = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('nbp\\.|nbpAnnual')]\n",
    "TRENDY_v12_S2_nbp = TRENDY_v12_S2_nbp[~TRENDY_v12_S2_nbp[0].str.contains('CARDAMOM')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 161.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CABLE-POP_S2_cCwd.nc.gz to ../data/CABLEPOP/S2\n",
      "Downloading CABLE-POP_S2_cLitter.nc.gz to ../data/CABLEPOP/S2\n",
      "Downloading CABLE-POP_S2_cSoil.nc.gz to ../data/CABLEPOP/S2\n",
      "Downloading CABLE-POP_S2_cVeg.nc.gz to ../data/CABLEPOP/S2\n",
      "Downloading CLASSIC_S2_cLitter.nc to ../data/CLASSIC/S2\n",
      "Downloading CLASSIC_S2_cSoil.nc to ../data/CLASSIC/S2\n",
      "Downloading CLASSIC_S2_cVeg.nc to ../data/CLASSIC/S2\n",
      "Downloading CLM5.0_S2_cCwd.nc to ../data/CLM5.0/S2\n",
      "Downloading CLM5.0_S2_cLitter.nc to ../data/CLM5.0/S2\n",
      "Downloading CLM5.0_S2_cProduct.nc to ../data/CLM5.0/S2\n",
      "Downloading CLM5.0_S2_cSoil.nc to ../data/CLM5.0/S2\n",
      "Downloading CLM5.0_S2_cVeg.nc to ../data/CLM5.0/S2\n",
      "Downloading DLEM_S2_cCwd.nc to ../data/DLEM/S2\n",
      "Downloading DLEM_S2_cLitter.nc to ../data/DLEM/S2\n",
      "Downloading DLEM_S2_cProduct.nc to ../data/DLEM/S2\n",
      "Downloading DLEM_S2_cSoil.nc to ../data/DLEM/S2\n",
      "Downloading DLEM_S2_cVeg.nc to ../data/DLEM/S2\n",
      "Downloading EDv3_S2_cProduct.nc to ../data/ED/S2\n",
      "Downloading EDv3_S2_cSoil.nc to ../data/ED/S2\n",
      "Downloading EDv3_S2_cVeg.nc to ../data/ED/S2\n",
      "Downloading E3SM_S2_cCwd.nc to ../data/ELM/S2\n",
      "Downloading E3SM_S2_cLitter.nc to ../data/ELM/S2\n",
      "Downloading E3SM_S2_cSoil.nc to ../data/ELM/S2\n",
      "Downloading E3SM_S2_cVeg.nc to ../data/ELM/S2\n",
      "Downloading IBIS_S2_cCwd.nc to ../data/IBIS/S2\n",
      "Downloading IBIS_S2_cLitter.nc to ../data/IBIS/S2\n",
      "Downloading IBIS_S2_cProduct.nc to ../data/IBIS/S2\n",
      "Downloading IBIS_S2_cSoil.nc to ../data/IBIS/S2\n",
      "Downloading IBIS_S2_cVeg.nc to ../data/IBIS/S2\n",
      "Downloading ISAM_S2_cSoil.nc to ../data/ISAM/S2\n",
      "Downloading ISAM_S2_cVeg.nc to ../data/ISAM/S2\n",
      "Downloading ISBA-CTRIP_S2_cLitter.nc to ../data/ISBACTRIP/S2\n",
      "Downloading ISBA-CTRIP_S2_cProduct.nc to ../data/ISBACTRIP/S2\n",
      "Downloading ISBA-CTRIP_S2_cSoil.nc to ../data/ISBACTRIP/S2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:00, 151.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ISBA-CTRIP_S2_cVeg.nc to ../data/ISBACTRIP/S2\n",
      "Downloading JSBACH_S2_cLitter.nc to ../data/JSBACH/S2\n",
      "Downloading JSBACH_S2_cProduct.nc to ../data/JSBACH/S2\n",
      "Downloading JSBACH_S2_cSoil.nc to ../data/JSBACH/S2\n",
      "Downloading JSBACH_S2_cVeg.nc to ../data/JSBACH/S2\n",
      "Downloading JULES_S2_cSoil.nc to ../data/JULES/S2\n",
      "Downloading JULES_S2_cVeg.nc to ../data/JULES/S2\n",
      "Downloading LPJ-GUESS_S2_cCwd.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJ-GUESS_S2_cLitter.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJ-GUESS_S2_cProduct.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJ-GUESS_S2_cSoil.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJ-GUESS_S2_cVeg.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJmL_S2_cCwd.nc to ../data/LPJml/S2\n",
      "Downloading LPJmL_S2_cLitter.nc to ../data/LPJml/S2\n",
      "Downloading LPJmL_S2_cProduct.nc to ../data/LPJml/S2\n",
      "Downloading LPJmL_S2_cSoil.nc to ../data/LPJml/S2\n",
      "Downloading LPJmL_S2_cVeg.nc to ../data/LPJml/S2\n",
      "Downloading LPJwsl_S2_cLitter.nc.gz to ../data/LPJwsl/S2\n",
      "Downloading LPJwsl_S2_cSoil.nc.gz to ../data/LPJwsl/S2\n",
      "Downloading LPJwsl_S2_cVeg.nc.gz to ../data/LPJwsl/S2\n",
      "Downloading OCN_S2_cLitter.nc to ../data/OCN/S2\n",
      "Downloading OCN_S2_cProduct.nc to ../data/OCN/S2\n",
      "Downloading OCN_S2_cSoil.nc to ../data/OCN/S2\n",
      "Downloading OCN_S2_cVeg.nc to ../data/OCN/S2\n",
      "Downloading ORCHIDEE_S2_cLitter.nc to ../data/ORCHIDEE/S2\n",
      "Downloading ORCHIDEE_S2_cProduct.nc to ../data/ORCHIDEE/S2\n",
      "Downloading ORCHIDEE_S2_cSoil.nc to ../data/ORCHIDEE/S2\n",
      "Downloading ORCHIDEE_S2_cVeg.nc to ../data/ORCHIDEE/S2\n",
      "Downloading SDGVM_S2_cLitter.nc to ../data/SDGVM/S2\n",
      "Downloading SDGVM_S2_cSoil.nc to ../data/SDGVM/S2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 153.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SDGVM_S2_cVeg.nc to ../data/SDGVM/S2\n",
      "Downloading VISIT_S2_cLitter.nc.gz to ../data/VISIT/S2\n",
      "Downloading VISIT_S2_cProduct.nc.gz to ../data/VISIT/S2\n",
      "Downloading VISIT_S2_cSoil.nc.gz to ../data/VISIT/S2\n",
      "Downloading VISIT_S2_cVeg.nc.gz to ../data/VISIT/S2\n",
      "Downloading YIBs_S2_Annual_cSoil.nc.tar.gz to ../data/YIBS/S2\n",
      "Downloading YIBs_S2_Annual_cVeg.nc.tar.gz to ../data/YIBS/S2\n",
      "Downloading LPX-Bern_S2_cLitter.nc to ../data/lpxqs/S2\n",
      "Downloading LPX-Bern_S2_cProduct.nc to ../data/lpxqs/S2\n",
      "Downloading LPX-Bern_S2_cSoil.nc to ../data/lpxqs/S2\n",
      "Downloading LPX-Bern_S2_cVeg.nc to ../data/lpxqs/S2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aws_url = 'https://gcbo-opendata.s3.eu-west-2.amazonaws.com/'\n",
    "\n",
    "for i, row in tqdm(TRENDY_v12_S2_cPools.iterrows()):\n",
    "    \n",
    "    # get the url for download\n",
    "    download_url = aws_url + row[0]\n",
    "\n",
    "    # define the destination directory and file\n",
    "    dst_dir = data_dir + '/'.join(row[0].split('/')[1:-1])\n",
    "    dst_file = row[0].split('/')[-1]\n",
    "\n",
    "    # if destination directory does not exist, create it\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "\n",
    "    # if file does not exist, download it\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: illegal option -- n\n",
      "usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression]\n",
      "       find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression]\n",
      "find: illegal option -- n\n",
      "usage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression]\n",
      "       find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression]\n",
      "mv: rename ../data//YIBs_S2_Annual_cSoil.nc to ../data//YIBS/S2/YIBs_S2_Annual_cSoil.nc: No such file or directory\n",
      "mv: rename ../data//YIBs_S2_Annual_cVeg.nc to ../data//YIBS/S2/YIBs_S2_Annual_cVeg.nc: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# uncomress all compressed files in the directories under ../data/\n",
    "! find $data_dir -name \"*.gz\" -exec gunzip {} \\;\n",
    "\n",
    "# uncomress all compressed tar files in the directories under ../data/ into the same directory\n",
    "! find $data_dir -name \"*.tar\" -exec tar -xvf {} -C ../data/ \\;\n",
    "\n",
    "! mv $data_dir/YIBs_S2_Annual_cSoil.nc $data_dir/YIBS/S2/\n",
    "! mv $data_dir/YIBs_S2_Annual_cVeg.nc $data_dir/YIBS/S2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 206.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CABLE-POP_S2_nbp.nc.gz to ../data/CABLEPOP/S2\n",
      "Downloading CLASSIC_S2_nbp.nc to ../data/CLASSIC/S2\n",
      "Downloading CLM5.0_S2_nbp.nc to ../data/CLM5.0/S2\n",
      "Downloading DLEM_S2_nbp.nc to ../data/DLEM/S2\n",
      "Downloading EDv3_S2_nbp.nc to ../data/ED/S2\n",
      "Downloading E3SM_S2_nbp.nc to ../data/ELM/S2\n",
      "Downloading IBIS_S2_nbp.nc to ../data/IBIS/S2\n",
      "Downloading ISAM_S2_nbp.nc to ../data/ISAM/S2\n",
      "Downloading ISBA-CTRIP_S2_nbp.nc to ../data/ISBACTRIP/S2\n",
      "Downloading JSBACH_S2_nbp.nc to ../data/JSBACH/S2\n",
      "Downloading JULES_S2_nbp.nc to ../data/JULES/S2\n",
      "Downloading LPJ-GUESS_S2_nbp.nc to ../data/LPJ-GUESS/S2\n",
      "Downloading LPJmL_S2_nbp.nc to ../data/LPJml/S2\n",
      "Downloading LPJwsl_S2_nbp.nc.gz to ../data/LPJwsl/S2\n",
      "Downloading OCN_S2_nbp.nc to ../data/OCN/S2\n",
      "Downloading ORCHIDEE_S2_nbp.nc to ../data/ORCHIDEE/S2\n",
      "Downloading SDGVM_S2_nbp.csv to ../data/SDGVM/S2\n",
      "Downloading SDGVM_S2_nbpAnnual.nc to ../data/SDGVM/S2\n",
      "Downloading VISIT_S2_nbp.nc.gz to ../data/VISIT/S2\n",
      "Downloading YIBs_S2_Monthly_nbp.nc.tar.gz to ../data/YIBS/S2\n",
      "Downloading LPX-Bern_S2_nbp.nc to ../data/lpxqs/S2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aws_url = 'https://gcbo-opendata.s3.eu-west-2.amazonaws.com/'\n",
    "for i, row in tqdm(TRENDY_v12_S2_nbp.iterrows()):\n",
    "    \n",
    "    # get the url for download\n",
    "    download_url = aws_url + row[0]\n",
    "\n",
    "    # define the destination directory and file\n",
    "    dst_dir = data_dir + '/'.join(row[0].split('/')[1:-1])\n",
    "    dst_file = row[0].split('/')[-1]\n",
    "\n",
    "    # if destination directory does not exist, create it\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "\n",
    "    # if file does not exist, download it\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: rename ../data/YIBs_S2_Annual_cSoil.nc to ../data//YIBS/S2/YIBs_S2_Annual_cSoil.nc: No such file or directory\n",
      "mv: rename ../data/YIBs_S2_Annual_cVeg.nc to ../data//YIBS/S2/YIBs_S2_Annual_cVeg.nc: No such file or directory\n",
      "mv: rename ../data/YIBs_S2_Monthly_nbp.nc to ../data//YIBS/S2/YIBs_S2_Monthly_nbp.nc: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# uncomress all compressed files in the directories under ../data/\n",
    "! find ../data/ -name \"*.gz\" -exec gunzip {} \\;\n",
    "\n",
    "# uncomress all compressed tar files in the directories under ../data/ into the same directory\n",
    "! find ../data/ -name \"*.tar\" -exec tar -xvf {} -C ../data/ \\;\n",
    "\n",
    "! mv ../data/YIBs_S2_Annual_cSoil.nc $data_dir/YIBS/S2/\n",
    "! mv ../data/YIBs_S2_Annual_cVeg.nc $data_dir/YIBS/S2/\n",
    "! mv  ../data/YIBs_S2_Monthly_nbp.nc $data_dir/YIBS/S2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download cell area files\n",
    "\n",
    "The `areacella_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc`, `sftlf_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc` and `JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc` files are from the CMIP outputs or from TRENDYv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CLASSIC_S2_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading CLASSIC_S2_land_fraction.nc to ../data//cell_area/\n",
      "Downloading CLM5.0_S2_area.nc to ../data//cell_area/\n",
      "Downloading EDv3_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading IBIS_S2_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading IBIS_S2_oceanCoverFrac.nc to ../data//cell_area/\n",
      "Downloading ISBA-CTRIP_S2_sftlf.nc to ../data//cell_area/\n",
      "Downloading LPJmL_S2_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading LPJmL_S2_oceanCoverFrac.nc to ../data//cell_area/\n",
      "Downloading OCN_S2_area.nc to ../data//cell_area/\n",
      "Downloading OCN_S2_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading OCN_S2_oceanCoverFrac.nc to ../data//cell_area/\n",
      "Downloading ORCHIDEE_S2_landCoverFrac.nc to ../data//cell_area/\n",
      "Downloading ORCHIDEE_S2_oceanCoverFrac.nc to ../data//cell_area/\n",
      "Downloading DLEM_land_area.nc to ../data//cell_area/\n",
      "Downloading ISBA-CTRIP_area.nc to ../data//cell_area/\n"
     ]
    }
   ],
   "source": [
    "cell_area_files = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('_area|_oceanCoverFrac\\.|_land_fraction\\.|_landCoverFrac\\.|sftlf\\.|landAreaFrac')]\n",
    "\n",
    "DLEM_area = 'trendyv11-gcb2022/DLEM/DLEM_land_area.nc'\n",
    "ISBACTRIP_area = 'trendyv12-gcb2023/ISBACTRIP/ISBA-CTRIP_area.nc'\n",
    "\n",
    "models = ['CLM5.0','IBIS','OCN','ORCHIDEE','LPJmL','CLASSIC','EDv3','ISBA-CTRIP']\n",
    "\n",
    "cell_area_files = cell_area_files[cell_area_files[0].str.contains('|'.join(models))]\n",
    "\n",
    "for file in list(cell_area_files[0].values) + [DLEM_area, ISBACTRIP_area]:\n",
    "    download_url = aws_url + file\n",
    "    dst_dir = data_dir + '/cell_area/'\n",
    "    dst_file = file.split('/')[-1]\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate surface area of each pixel\n",
    "def calc_pixel_area(raster:xr.DataArray) -> xr.DataArray:\n",
    "    '''\n",
    "    Calculate the area of each pixel in a raster\n",
    "\n",
    "    Parameters:\n",
    "    raster (xarray.DataArray): raster to calculate pixel area for\n",
    "\n",
    "    Returns:\n",
    "    xarray.DataArray: raster with pixel area as values\n",
    "    '''\n",
    "\n",
    "    # get the resolution of the raster\n",
    "    res = raster.rio.resolution()\n",
    "\n",
    "    l1 = np.radians(raster['y']- np.abs(res[1])/2)\n",
    "    l2 = np.radians(raster['y']+ np.abs(res[1])/2)\n",
    "    dx = np.radians(np.abs(res[0]))    \n",
    "    _R = 6371e3  # Radius of earth in m. Use 3956e3 for miles\n",
    "\n",
    "    # calculate the area of each pixel\n",
    "    area = _R**2 * dx * (np.sin(l2) - np.sin(l1))\n",
    "\n",
    "    # create a new xarray with the pixel area as values\n",
    "    result = ((raster-raster+1)*area)\n",
    "\n",
    "    # set the nodata value    \n",
    "    if raster.rio.nodata is None:\n",
    "        result.rio.set_nodata(np.nan,inplace=True)\n",
    "    else:\n",
    "        result.rio.set_nodata(raster.rio.nodata,inplace=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(model:str) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Get the area of each pixel for a given model\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray\n",
    "        the area of each pixel\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'CLM5.0':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area_ds = xr.open_dataset(f'{data_dir}/cell_area/CLM5.0_S2_area.nc')\n",
    "        area = area_ds['area']*area_ds['landfrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model == 'DLEM':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/DLEM_land_area.nc')['LAND_AREA']*1e6\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model in ['IBIS','OCN','ORCHIDEE','LPJml']:\n",
    "        if model == \"LPJml\":\n",
    "            model = \"LPJmL\"\n",
    "        # load the ocean cover fraction data\n",
    "        ocean = xr.open_dataset(f'{data_dir}/cell_area/{model}_S2_oceanCoverFrac.nc',decode_times=False)['oceanCoverFrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        ocean = ocean.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        \n",
    "        # the land data is the cell area times the fraction of the cell that is not ocean\n",
    "        area = calc_pixel_area(ocean)*(1-ocean)\n",
    "    elif model == 'CLASSIC':\n",
    "        # load land fraction data\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/CLASSIC_S2_land_fraction.nc')['sftlf'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "    elif model == 'ED':\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/EDv3_landCoverFrac.nc')['landArea'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "    elif model == 'ELM':\n",
    "\n",
    "        # load the a file with the base resolution to reproject the area onto\n",
    "        ds = xr.open_dataset(glob(f'../data/{\"ELM\"}/S2/*{\"nbp\"}*.nc')[0],decode_times=False)\n",
    "\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "\n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "        ds = ds.swap_dims({'lon':'x','lat':'y'})\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "        ds.rio.write_crs('EPSG:4326',inplace=True)\n",
    "        ds = ds['nbp'][0,:,:]\n",
    "\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/areacella_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['areacella'].rename({'lat': 'y', 'lon': 'x'})\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/sftlf_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['sftlf'].rename({'lat': 'y', 'lon': 'x'})/100\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "\n",
    "        # change the coordinates to start from -180 to 180\n",
    "        area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "        area = area.sortby(['x','y'])\n",
    "        \n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "\n",
    "        # reproject the area into the base resolution\n",
    "        area = area.rio.write_crs('EPSG:4326',inplace=True).rio.reproject_match(ds,resampling=Resampling.sum)\n",
    "        area = area.where(area<1e30)\n",
    "\n",
    "    elif model == 'ISBACTRIP':\n",
    "        # load the grid cell area file and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_area.nc')['AREA'].rename({'LAT_FULL':'y','LON_FULL':'x'})\n",
    "        \n",
    "        # load the land area fraction data and rename coordinates\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_S2_sftlf.nc',decode_times=False)['sftlf'].mean(dim='time_counter').rename({'lat_FULL':'y','lon_FULL':'x'})\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "    elif model == 'JULES':\n",
    "        \n",
    "        # load the lancdAreaFrac from trendy-v10\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc')['landFrac']\n",
    "        \n",
    "        # renanme the coordinates\n",
    "        land_fraction = land_fraction.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "        # the land area is the grid cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "\n",
    "    return area\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model(model:str,var:str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Parse the data for a given model and variable\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "    var: str\n",
    "        the name of the variable\n",
    "\n",
    "    Returns:\n",
    "    xr.Dataset\n",
    "        the parsed data\n",
    "    \"\"\"\n",
    "\n",
    "    # open the dataset\n",
    "    ds = xr.open_dataset(glob(f'../data/{model}/S2/*{var}*.nc')[0],decode_times=False)\n",
    "\n",
    "    # convert coordinates to standard time,y,x\n",
    "    if 'time' not in ds.sizes.keys():\n",
    "       ds = ds.rename({'time_counter':'time'}) \n",
    "    if 'lon' in ds.dims:\n",
    "        ds = ds.rename({'lon':'x','lat':'y'})\n",
    "    elif 'longitude' in ds.dims:\n",
    "        ds = ds.rename({'longitude':'x','latitude':'y'})\n",
    "    else:\n",
    "        ds = ds.rename({'lon_FULL':'x','lat_FULL':'y'})\n",
    "    \n",
    "    # set the time coordinate to datetime based on the size of the file\n",
    "    if ds.sizes['time'] == 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1860', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] > 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] >300:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='YS')\n",
    "    else:\n",
    "        ds['time'] = pd.date_range(start='01-01-2002', periods=len(ds.time), freq='MS')\n",
    "    \n",
    "    if 'nbnd' in ds.dims:\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "        \n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "    if 'bnds' in ds.dims:\n",
    "        # if the dataset had a bnds coordinate drop it\n",
    "        ds = ds.drop_dims('bnds')\n",
    "\n",
    "    # order the coordinates\n",
    "    ds = ds.transpose('time','y','x')\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "\n",
    "    # if the data is in the 0-360 range, convert it to -180-180\n",
    "    if ds['x'].min()>=0:\n",
    "        ds.coords['x'] = xr.where(ds.coords['x']>=180, ds.coords['x']-360, ds.coords['x'])\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "    \n",
    "    # if the variable is not in the data_vars, rename it\n",
    "    ds_var = list(ds.data_vars.keys())[0]\n",
    "    if var not in ds.data_vars:\n",
    "        ds = ds.rename({ds_var:var})\n",
    "\n",
    "    # # get the land area of each pixel\n",
    "\n",
    "    # # define the models that need special attention\n",
    "    models_to_fix = ['CLM5.0','DLEM','IBIS','OCN','ORCHIDEE','ISBACTRIP','JULES','CLASSIC','ED','ELM','LPJml']\n",
    "\n",
    "    # if the model needs special attention, use the get_area function to calculate the land area\n",
    "    if model in models_to_fix:\n",
    "        area = get_area(model)\n",
    "        if 'time' in area.dims:\n",
    "            area = area.sel(time=area['time'][0]).drop_vars('time')\n",
    "        if area['x'].min()>=0:\n",
    "            area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "    else:\n",
    "\n",
    "        # otherwise use the calc_pixel_area function to calculate the land area\n",
    "        area = calc_pixel_area(ds[var][1,:,:])\n",
    "\n",
    "    # name the land_area DataArray\n",
    "    area.name = 'land_area'\n",
    "\n",
    "    # take the annual average of the data\n",
    "    ds = ds.resample(time='YS').mean()\n",
    "    \n",
    "    if model in ['CLASSIC','CLM5.0']:\n",
    "        # if the model is CLASSIC or CLM5.0, shift years by one\n",
    "        ds['time'] = (ds['time'].to_series() + pd.DateOffset(years=1)).values\n",
    "\n",
    "    # return a merged dataset of the data and the land area\n",
    "    # result = xr.merge([ds,area])\n",
    "\n",
    "    # return result\n",
    "    return ds,area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate nbp against GCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "must supply at least one object to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/utils.py\u001b[0m in \u001b[0;36mpeek_at\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d5b4f81c542d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# merge the datasets along the pool dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel_merged_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# extract the model name from the file name and not the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must supply at least one object to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_VALID_COMPAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: must supply at least one object to concatenate"
     ]
    }
   ],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_nbp = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "    \n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "    \n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "\n",
    "        # if the variable is a nbp variable, parse the model\n",
    "        if var in ['nbp','nbpAnnual']:\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar = parse_model(model,var)\n",
    "\n",
    "            # add the product of the nbp and surface area to get units of KgC s-1 per gridcell\n",
    "            model_dss.append(dss[var]*ar)\n",
    "    \n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('/')[-1].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "\n",
    "    # calculate the global nbp - convert from kgC s-1 to PgC yr-1\n",
    "    global_nbp = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15 * 365*24*3600\n",
    "    global_nbp.name = model2\n",
    "    \n",
    "    # append the global nbp to the list\n",
    "    parsed_global_nbp.append(global_nbp)\n",
    "\n",
    "# merge the global nbp datasets along the model dimension\n",
    "models_global_nbp = xr.concat(parsed_global_nbp,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_nbp['model'] = [x.name for x in parsed_global_nbp]\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_nbp.name = 'nbp'\n",
    "models_global_nbp_df = models_global_nbp.to_dataframe()['nbp'].unstack()\n",
    "models_global_nbp_df.columns = models_global_nbp_df.columns.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the GCB2023 data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m GCB \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://globalcarbonbudgetdata.org/downloads/archive/Global_Carbon_Budget_2023v1.1.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerrestrial Sink\u001b[39m\u001b[38;5;124m'\u001b[39m,skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m)\n\u001b[1;32m      3\u001b[0m GCB\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m GCB \u001b[38;5;241m=\u001b[39m GCB\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load the GCB2023 data\n",
    "GCB = pd.read_excel('https://globalcarbonbudgetdata.org/downloads/archive/Global_Carbon_Budget_2023v1.1.xlsx',sheet_name='Terrestrial Sink',skiprows=27)\n",
    "GCB.set_index('Year',inplace=True)\n",
    "GCB = GCB.iloc[:,2:-3]\n",
    "\n",
    "# change the model names to match our analysis\n",
    "models_GCB = list(GCB.columns)\n",
    "models_GCB[4] = 'EDv3'\n",
    "models_GCB[5] = 'E3SM'\n",
    "models_GCB[10] = 'JULES'\n",
    "models_GCB[11] = 'LPJ-GUESS'\n",
    "models_GCB[13] = 'LPJmL'\n",
    "models_GCB[15] = 'OCN'\n",
    "models_GCB[16] = 'ORCHIDEE'\n",
    "GCB.columns = models_GCB\n",
    "\n",
    "# assert that the RMSE for all models is less than 7%\n",
    "assert all((((models_global_nbp_df.T.loc[1959:2022] - GCB.loc[1959:2022])**2).mean()**0.5/GCB.loc[1959:2022].mean()*100).dropna().round(2).values < 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis for soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "must supply at least one object to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/utils.py\u001b[0m in \u001b[0;36mpeek_at\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-28d82a43bf5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# merge the datasets along the pool dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel_merged_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# extract the model name from the file name and not the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xarray/core/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mfirst_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must supply at least one object to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_VALID_COMPAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: must supply at least one object to concatenate"
     ]
    }
   ],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_cSoil = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "\n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "\n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        if var in ['cCwd','cLitter','cSoil']:\n",
    "            \n",
    "            # based on O'Sullivan 2022 et al. for CLM5 cLitter is included in cSoil and for CABLEPOP cCwd is included in cLitter\n",
    "            if (model == 'CLM5.0' and var == 'cLitter') or (model == 'CABLEPOP' and var == 'cCwd'):\n",
    "                continue\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar = parse_model(model,var)\n",
    "\n",
    "            # add the product of the nbp and surface area to get units of KgC per gridcell\n",
    "            model_dss.append(dss[var]*ar)\n",
    "\n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "    \n",
    "    # calculate the global stocks - convert from kgC to PgC\n",
    "    global_cSoil = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15\n",
    "    global_cSoil.name = model2\n",
    "    \n",
    "    # append the global stocks to the list\n",
    "    parsed_global_cSoil.append(global_cSoil)\n",
    "\n",
    "# merge the global stocks datasets along the model dimension\n",
    "models_global_cSoil = xr.concat(parsed_global_cSoil,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_cSoil['model'] = [x.name for x in parsed_global_cSoil]\n",
    "models_global_cSoil.name = 'cSoil'\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_cSoil_df = models_global_cSoil.to_dataframe()['cSoil'].unstack()\n",
    "models_global_cSoil_df.columns = models_global_cSoil_df.columns.year\n",
    "\n",
    "# save the dataframes to csv\n",
    "models_global_cSoil_df.to_csv('../results/TRENDY_v12_global_cSoil_S2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average SOC stock change from 1992 to 2022 for the TRENDY v12 models is 1.07 PgC yr-1\n"
     ]
    }
   ],
   "source": [
    "SOC_stock_change = models_global_cSoil_df.diff(axis=1).loc[:,1992:2022].mean(axis=1).mean()\n",
    "\n",
    "print(f'The average SOC stock change from 1992 to 2022 for the TRENDY v12 models is {SOC_stock_change:.2f} PgC yr-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

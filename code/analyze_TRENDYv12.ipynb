{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for analysis of the soil carbon sink in TRENDYv12 S2 simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relevant files from Mike O'Sullivan's gihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1294877/979931139.py:4: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  TRENDY_FILES = pd.read_json(file_index.read().decode('utf-8'))\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "url = 'https://raw.githubusercontent.com/mdosullivan/GCB/main/fileIndex.json'\n",
    "file_index = urlopen(url)\n",
    "TRENDY_FILES = pd.read_json(file_index.read().decode('utf-8'))\n",
    "\n",
    "# Take only the TRENDY v12 files from the S2 simulation, and only the carbon pools. Exclude CARDAMOM.\n",
    "TRENDY_v12 = TRENDY_FILES[TRENDY_FILES[0].str.contains('trendyv12')]\n",
    "TRENDY_v12_S2 = TRENDY_v12[TRENDY_v12[0].str.contains('/S2/')]\n",
    "TRENDY_v12_S2_cPools = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('_cSoil\\.|cVeg\\.|cLitter\\.|cCwd\\.|cProduct\\.')]\n",
    "TRENDY_v12_S2_cPools = TRENDY_v12_S2_cPools[~TRENDY_v12_S2_cPools[0].str.contains('CARDAMOM')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRENDY_v12_S2_nbp = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('nbp\\.|nbpAnnual')]\n",
    "TRENDY_v12_S2_nbp = TRENDY_v12_S2_nbp[~TRENDY_v12_S2_nbp[0].str.contains('CARDAMOM')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_url = 'https://gcbo-opendata.s3.eu-west-2.amazonaws.com/'\n",
    "\n",
    "for i, row in tqdm(TRENDY_v12_S2_cPools.iterrows()):\n",
    "    \n",
    "    # get the url for download\n",
    "    download_url = aws_url + row[0]\n",
    "\n",
    "    # define the destination directory and file\n",
    "    dst_dir = data_dir + '/'.join(row[0].split('/')[1:-1])\n",
    "    dst_file = row[0].split('/')[-1]\n",
    "\n",
    "    # if destination directory does not exist, create it\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "\n",
    "    # if file does not exist, download it\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomress all compressed files in the directories under ../data/\n",
    "! find $data_dir -name \"*.gz\" -exec gunzip {} \\;\n",
    "\n",
    "# uncomress all compressed tar files in the directories under ../data/ into the same directory\n",
    "! find $data_dir -name \"*.tar\" -exec tar -xvf {} -C ../data/ \\;\n",
    "\n",
    "! mv $data_dir/YIBs_S2_Annual_cSoil.nc $data_dir/YIBS/S2/\n",
    "! mv $data_dir/YIBs_S2_Annual_cVeg.nc $data_dir/YIBS/S2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_url = 'https://gcbo-opendata.s3.eu-west-2.amazonaws.com/'\n",
    "for i, row in tqdm(TRENDY_v12_S2_nbp.iterrows()):\n",
    "    \n",
    "    # get the url for download\n",
    "    download_url = aws_url + row[0]\n",
    "\n",
    "    # define the destination directory and file\n",
    "    dst_dir = data_dir + '/'.join(row[0].split('/')[1:-1])\n",
    "    dst_file = row[0].split('/')[-1]\n",
    "\n",
    "    # if destination directory does not exist, create it\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "\n",
    "    # if file does not exist, download it\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YIBs_S2_Monthly_nbp.nc\n",
      "mv: cannot stat '../data/YIBs_S2_Annual_cSoil.nc': No such file or directory\n",
      "mv: cannot stat '../data/YIBs_S2_Annual_cVeg.nc': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# uncomress all compressed files in the directories under ../data/\n",
    "! find ../data/ -name \"*.gz\" -exec gunzip {} \\;\n",
    "\n",
    "# uncomress all compressed tar files in the directories under ../data/ into the same directory\n",
    "! find ../data/ -name \"*.tar\" -exec tar -xvf {} -C ../data/ \\;\n",
    "\n",
    "! mv ../data/YIBs_S2_Annual_cSoil.nc $data_dir/YIBS/S2/\n",
    "! mv ../data/YIBs_S2_Annual_cVeg.nc $data_dir/YIBS/S2/\n",
    "! mv  ../data/YIBs_S2_Monthly_nbp.nc $data_dir/YIBS/S2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download cell area files\n",
    "\n",
    "The `areacella_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc`, `sftlf_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc` and `JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc` files are from the CMIP outputs or from TRENDYv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ISBA-CTRIP_area.nc to ../data//cell_area/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-01-02 15:35:58--  https://gcbo-opendata.s3.eu-west-2.amazonaws.com/trendyv12-gcb2023/ISBACTRIP/ISBA-CTRIP_area.nc\n",
      "Resolving gcbo-opendata.s3.eu-west-2.amazonaws.com (gcbo-opendata.s3.eu-west-2.amazonaws.com)... 3.5.246.158, 3.5.245.110, 52.95.191.30, ...\n",
      "Connecting to gcbo-opendata.s3.eu-west-2.amazonaws.com (gcbo-opendata.s3.eu-west-2.amazonaws.com)|3.5.246.158|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 436832 (427K) [application/x-netcdf]\n",
      "Saving to: ‘../data//cell_area/ISBA-CTRIP_area.nc’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 11%  374K 1s\n",
      "    50K .......... .......... .......... .......... .......... 23%  376K 1s\n",
      "   100K .......... .......... .......... .......... .......... 35% 16.4M 0s\n",
      "   150K .......... .......... .......... .......... .......... 46%  389K 0s\n",
      "   200K .......... .......... .......... .......... .......... 58% 15.6M 0s\n",
      "   250K .......... .......... .......... .......... .......... 70% 18.6M 0s\n",
      "   300K .......... .......... .......... .......... .......... 82%  112M 0s\n",
      "   350K .......... .......... .......... .......... .......... 93%  420K 0s\n",
      "   400K .......... .......... ......                          100% 3.28M=0.5s\n",
      "\n",
      "2025-01-02 15:35:59 (803 KB/s) - ‘../data//cell_area/ISBA-CTRIP_area.nc’ saved [436832/436832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cell_area_files = TRENDY_v12_S2[TRENDY_v12_S2[0].str.contains('_area|_oceanCoverFrac\\.|_land_fraction\\.|_landCoverFrac\\.|sftlf\\.|landAreaFrac')]\n",
    "\n",
    "DLEM_area = 'trendyv11-gcb2022/DLEM/DLEM_land_area.nc'\n",
    "ISBACTRIP_area = 'trendyv12-gcb2023/ISBACTRIP/ISBA-CTRIP_area.nc'\n",
    "\n",
    "models = ['CLM5.0','IBIS','OCN','ORCHIDEE','LPJmL','CLASSIC','EDv3','ISBA-CTRIP']\n",
    "\n",
    "cell_area_files = cell_area_files[cell_area_files[0].str.contains('|'.join(models))]\n",
    "\n",
    "for file in list(cell_area_files[0].values) + [DLEM_area, ISBACTRIP_area]:\n",
    "    download_url = aws_url + file\n",
    "    dst_dir = data_dir + '/cell_area/'\n",
    "    dst_file = file.split('/')[-1]\n",
    "    if not os.path.exists(dst_dir + '/' + dst_file):\n",
    "        print(f'Downloading {dst_file} to {dst_dir}')\n",
    "        os.system(f'wget {download_url} -P {dst_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate surface area of each pixel\n",
    "def calc_pixel_area(raster:xr.DataArray) -> xr.DataArray:\n",
    "    '''\n",
    "    Calculate the area of each pixel in a raster\n",
    "\n",
    "    Parameters:\n",
    "    raster (xarray.DataArray): raster to calculate pixel area for\n",
    "\n",
    "    Returns:\n",
    "    xarray.DataArray: raster with pixel area as values\n",
    "    '''\n",
    "\n",
    "    # get the resolution of the raster\n",
    "    res = raster.rio.resolution()\n",
    "\n",
    "    l1 = np.radians(raster['y']- np.abs(res[1])/2)\n",
    "    l2 = np.radians(raster['y']+ np.abs(res[1])/2)\n",
    "    dx = np.radians(np.abs(res[0]))    \n",
    "    _R = 6371e3  # Radius of earth in m. Use 3956e3 for miles\n",
    "\n",
    "    # calculate the area of each pixel\n",
    "    area = _R**2 * dx * (np.sin(l2) - np.sin(l1))\n",
    "\n",
    "    # create a new xarray with the pixel area as values\n",
    "    result = ((raster-raster+1)*area)\n",
    "\n",
    "    # set the nodata value    \n",
    "    if raster.rio.nodata is None:\n",
    "        result.rio.set_nodata(np.nan,inplace=True)\n",
    "    else:\n",
    "        result.rio.set_nodata(raster.rio.nodata,inplace=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(model:str) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Get the area of each pixel for a given model\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray\n",
    "        the area of each pixel\n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'CLM5.0':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area_ds = xr.open_dataset(f'{data_dir}/cell_area/CLM5.0_S2_area.nc')\n",
    "        area = area_ds['area']*area_ds['landfrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model == 'DLEM':\n",
    "        \n",
    "        # if the model is DLEM use the land area file and convert km2 to m2\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/DLEM_land_area.nc')['LAND_AREA']*1e6\n",
    "        area = area.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif model in ['IBIS','OCN','ORCHIDEE','LPJml']:\n",
    "        if model == \"LPJml\":\n",
    "            model = \"LPJmL\"\n",
    "        # load the ocean cover fraction data\n",
    "        ocean = xr.open_dataset(f'{data_dir}/cell_area/{model}_S2_oceanCoverFrac.nc',decode_times=False)['oceanCoverFrac']\n",
    "        \n",
    "        # rename coordinates to x,y\n",
    "        ocean = ocean.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        \n",
    "        # the land data is the cell area times the fraction of the cell that is not ocean\n",
    "        area = calc_pixel_area(ocean)*(1-ocean)\n",
    "    elif model == 'CLASSIC':\n",
    "        # load land fraction data\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/CLASSIC_S2_land_fraction.nc')['sftlf'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "    elif model == 'ED':\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        area = xr.open_dataset(f'{data_dir}/cell_area/EDv3_landCoverFrac.nc')['landArea'].rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "    elif model == 'ELM':\n",
    "\n",
    "        # load the a file with the base resolution to reproject the area onto\n",
    "        ds = xr.open_dataset(glob(f'../data/{\"ELM\"}/S2/*{\"nbp\"}*.nc')[0],decode_times=False)\n",
    "\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "\n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "        ds = ds.swap_dims({'lon':'x','lat':'y'})\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "        ds.rio.write_crs('EPSG:4326',inplace=True)\n",
    "        ds = ds['nbp'][0,:,:]\n",
    "\n",
    "        # load the land area fraction data and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/areacella_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['areacella'].rename({'lat': 'y', 'lon': 'x'})\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/sftlf_fx_E3SM-2-0_piControl_r1i1p1f1_gr.nc')['sftlf'].rename({'lat': 'y', 'lon': 'x'})/100\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "\n",
    "        # change the coordinates to start from -180 to 180\n",
    "        area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "        area = area.sortby(['x','y'])\n",
    "        \n",
    "        # order the coordinates\n",
    "        area = area.transpose('y','x')\n",
    "\n",
    "        # reproject the area into the base resolution\n",
    "        area = area.rio.write_crs('EPSG:4326',inplace=True).rio.reproject_match(ds,resampling=Resampling.sum)\n",
    "        area = area.where(area<1e30)\n",
    "\n",
    "    elif model == 'ISBACTRIP':\n",
    "        # load the grid cell area file and rename coordinates\n",
    "        cell_area = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_area.nc')['AREA'].rename({'LAT_FULL':'y','LON_FULL':'x'})\n",
    "        \n",
    "        # load the land area fraction data and rename coordinates\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/ISBA-CTRIP_S2_sftlf.nc',decode_times=False)['sftlf'].mean(dim='time_counter').rename({'lat_FULL':'y','lon_FULL':'x'})\n",
    "\n",
    "        # the land area is the cell area times the land fraction\n",
    "        area = cell_area*land_fraction\n",
    "    elif model == 'JULES':\n",
    "        \n",
    "        # load the lancdAreaFrac from trendy-v10\n",
    "        land_fraction = xr.open_dataset(f'{data_dir}/cell_area/JULES-ES.1p0.vn5.4.50.CRUJRA2.TRENDYv8.365.landAreaFrac.nc')['landFrac']\n",
    "        \n",
    "        # renanme the coordinates\n",
    "        land_fraction = land_fraction.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "        # the land area is the grid cell area times the land fraction\n",
    "        area = calc_pixel_area(land_fraction)*land_fraction\n",
    "\n",
    "    return area\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model(model:str,var:str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Parse the data for a given model and variable\n",
    "\n",
    "    Parameters:\n",
    "    model: str\n",
    "        the name of the model\n",
    "    var: str\n",
    "        the name of the variable\n",
    "\n",
    "    Returns:\n",
    "    xr.Dataset\n",
    "        the parsed data\n",
    "    \"\"\"\n",
    "\n",
    "    # open the dataset\n",
    "    ds = xr.open_dataset(glob(f'../data/{model}/S2/*{var}*.nc')[0],decode_times=False)\n",
    "\n",
    "    # convert coordinates to standard time,y,x\n",
    "    if 'time' not in ds.sizes.keys():\n",
    "       ds = ds.rename({'time_counter':'time'}) \n",
    "    if 'lon' in ds.dims:\n",
    "        ds = ds.rename({'lon':'x','lat':'y'})\n",
    "    elif 'longitude' in ds.dims:\n",
    "        ds = ds.rename({'longitude':'x','latitude':'y'})\n",
    "    else:\n",
    "        ds = ds.rename({'lon_FULL':'x','lat_FULL':'y'})\n",
    "    \n",
    "    # set the time coordinate to datetime based on the size of the file\n",
    "    if ds.sizes['time'] == 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1860', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] > 1956:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='MS')\n",
    "    elif ds.sizes['time'] >300:\n",
    "        ds['time'] = pd.date_range(start='01-01-1700', periods=len(ds.time), freq='YS')\n",
    "    else:\n",
    "        ds['time'] = pd.date_range(start='01-01-2002', periods=len(ds.time), freq='MS')\n",
    "    \n",
    "    if 'nbnd' in ds.dims:\n",
    "        # replace the x and y coordinates with the new ones\n",
    "        ds.coords['x'] = ds['longitude']\n",
    "        ds.coords['y'] = ds['latitude']\n",
    "        \n",
    "        # drop the nbnd coordinate\n",
    "        ds = ds.drop_dims('nbnd')\n",
    "\n",
    "        # drop the old longitude and latitude coordinates\n",
    "        ds = ds.drop_vars(['longitude','latitude'])\n",
    "\n",
    "    if 'bnds' in ds.dims:\n",
    "        # if the dataset had a bnds coordinate drop it\n",
    "        ds = ds.drop_dims('bnds')\n",
    "\n",
    "    # order the coordinates\n",
    "    ds = ds.transpose('time','y','x')\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "\n",
    "    # if the data is in the 0-360 range, convert it to -180-180\n",
    "    if ds['x'].min()>=0:\n",
    "        ds.coords['x'] = xr.where(ds.coords['x']>=180, ds.coords['x']-360, ds.coords['x'])\n",
    "    \n",
    "    # sort the data based on y and x\n",
    "    ds = ds.sortby(['y','x'])\n",
    "    \n",
    "    # if the variable is not in the data_vars, rename it\n",
    "    ds_var = list(ds.data_vars.keys())[0]\n",
    "    if var not in ds.data_vars:\n",
    "        ds = ds.rename({ds_var:var})\n",
    "\n",
    "    # # get the land area of each pixel\n",
    "\n",
    "    # # define the models that need special attention\n",
    "    models_to_fix = ['CLM5.0','DLEM','IBIS','OCN','ORCHIDEE','ISBACTRIP','JULES','CLASSIC','ED','ELM','LPJml']\n",
    "\n",
    "    # if the model needs special attention, use the get_area function to calculate the land area\n",
    "    if model in models_to_fix:\n",
    "        area = get_area(model)\n",
    "        if 'time' in area.dims:\n",
    "            area = area.sel(time=area['time'][0]).drop_vars('time')\n",
    "        if area['x'].min()>=0:\n",
    "            area.coords['x'] = xr.where(area.coords['x']>=180, area.coords['x']-360, area.coords['x'])\n",
    "    else:\n",
    "\n",
    "        # otherwise use the calc_pixel_area function to calculate the land area\n",
    "        area = calc_pixel_area(ds[var][1,:,:])\n",
    "\n",
    "    # name the land_area DataArray\n",
    "    area.name = 'land_area'\n",
    "\n",
    "    # take the annual average of the data\n",
    "    ds = ds.resample(time='YS').mean()\n",
    "    \n",
    "    if model in ['CLASSIC','CLM5.0']:\n",
    "        # if the model is CLASSIC or CLM5.0, shift years by one\n",
    "        ds['time'] = (ds['time'].to_series() + pd.DateOffset(years=1)).values\n",
    "\n",
    "    # return a merged dataset of the data and the land area\n",
    "    # result = xr.merge([ds,area])\n",
    "\n",
    "    # return result\n",
    "    return ds,area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate nbp against GCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_nbp = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "    \n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "    \n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "\n",
    "        # if the variable is a nbp variable, parse the model\n",
    "        if var in ['nbp','nbpAnnual']:\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar = parse_model(model,var)\n",
    "\n",
    "            # add the product of the nbp and surface area to get units of KgC s-1 per gridcell\n",
    "            model_dss.append(dss[var]*ar)\n",
    "    \n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('/')[-1].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "\n",
    "    # calculate the global nbp - convert from kgC s-1 to PgC yr-1\n",
    "    global_nbp = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15 * 365*24*3600\n",
    "    global_nbp.name = model2\n",
    "    \n",
    "    # append the global nbp to the list\n",
    "    parsed_global_nbp.append(global_nbp)\n",
    "\n",
    "# merge the global nbp datasets along the model dimension\n",
    "models_global_nbp = xr.concat(parsed_global_nbp,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_nbp['model'] = [x.name for x in parsed_global_nbp]\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_nbp.name = 'nbp'\n",
    "models_global_nbp_df = models_global_nbp.to_dataframe()['nbp'].unstack()\n",
    "models_global_nbp_df.columns = models_global_nbp_df.columns.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GCB2023 data\n",
    "GCB = pd.read_excel('https://globalcarbonbudgetdata.org/downloads/archive/Global_Carbon_Budget_2023v1.1.xlsx',sheet_name='Terrestrial Sink',skiprows=27)\n",
    "GCB.set_index('Year',inplace=True)\n",
    "GCB = GCB.iloc[:,2:-3]\n",
    "\n",
    "# change the model names to match our analysis\n",
    "models_GCB = list(GCB.columns)\n",
    "models_GCB[4] = 'EDv3'\n",
    "models_GCB[5] = 'E3SM'\n",
    "models_GCB[10] = 'JULES'\n",
    "models_GCB[11] = 'LPJ-GUESS'\n",
    "models_GCB[13] = 'LPJmL'\n",
    "models_GCB[15] = 'OCN'\n",
    "models_GCB[16] = 'ORCHIDEE'\n",
    "GCB.columns = models_GCB\n",
    "\n",
    "# assert that the RMSE for all models is less than 7%\n",
    "assert all((((models_global_nbp_df.T.loc[1959:2022] - GCB.loc[1959:2022])**2).mean()**0.5/GCB.loc[1959:2022].mean()*100).dropna().round(2).values < 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis for soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the directories with S2 subdirectories\n",
    "dirs = ! find ../data/ -name \"S2\";\n",
    "\n",
    "# extract the model names\n",
    "models = [x.split('/')[-2] for x in dirs]\n",
    "\n",
    "# initialize an empty list to store the parsed datasets\n",
    "parsed_ds = []\n",
    "parsed_global_cSoil = []\n",
    "\n",
    "# loop through the directories\n",
    "for dir in tqdm(dirs):\n",
    "\n",
    "    # extract the model name\n",
    "    model = dir.split('/')[-2]\n",
    "\n",
    "    # initialize an empty list to store the parsed datasets\n",
    "    model_dss = []\n",
    "\n",
    "    # find all of the netcdf files in the S2 directory\n",
    "    file_names = glob(dir + \"/*.nc\")\n",
    "\n",
    "    # loop through the files\n",
    "    for file in file_names:\n",
    "\n",
    "        # extract the variable name\n",
    "        var = file.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        if var in ['cCwd','cLitter','cSoil']:\n",
    "            \n",
    "            # based on O'Sullivan 2022 et al. for CLM5 cLitter is included in cSoil and for CABLEPOP cCwd is included in cLitter\n",
    "            if (model == 'CLM5.0' and var == 'cLitter') or (model == 'CABLEPOP' and var == 'cCwd'):\n",
    "                continue\n",
    "\n",
    "            # get the parsed model and area\n",
    "            dss,ar = parse_model(model,var)\n",
    "\n",
    "            # add the product of the nbp and surface area to get units of KgC per gridcell\n",
    "            model_dss.append(dss[var]*ar)\n",
    "\n",
    "    # merge the datasets along the pool dimension\n",
    "    model_merged_ds = xr.concat(model_dss,dim='pool')\n",
    "\n",
    "    # extract the model name from the file name and not the directory\n",
    "    model2 = file_names[0].split('_')[0]\n",
    "    model_merged_ds.name = model2\n",
    "    \n",
    "    # calculate the global stocks - convert from kgC to PgC\n",
    "    global_cSoil = model_merged_ds.sum(dim=['x','y','pool'])*1e3/1e15\n",
    "    global_cSoil.name = model2\n",
    "    \n",
    "    # append the global stocks to the list\n",
    "    parsed_global_cSoil.append(global_cSoil)\n",
    "\n",
    "# merge the global stocks datasets along the model dimension\n",
    "models_global_cSoil = xr.concat(parsed_global_cSoil,dim='model')\n",
    "\n",
    "# set the values of the model dimension to the model names\n",
    "models_global_cSoil['model'] = [x.name for x in parsed_global_cSoil]\n",
    "models_global_cSoil.name = 'cSoil'\n",
    "\n",
    "# convert the xarray to a dataframe\n",
    "models_global_cSoil_df = models_global_cSoil.to_dataframe()['cSoil'].unstack()\n",
    "models_global_cSoil_df.columns = models_global_cSoil_df.columns.year\n",
    "\n",
    "# save the dataframes to csv\n",
    "models_global_cSoil_df.to_csv('../results/TRENDY_v12_global_cSoil_S2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average SOC stock change from 1992 to 2022 for the TRENDY v12 models is 1.07 PgC yr-1\n"
     ]
    }
   ],
   "source": [
    "SOC_stock_change = models_global_cSoil_df.diff(axis=1).loc[:,1992:2022].mean(axis=1).mean()\n",
    "\n",
    "print(f'The average SOC stock change from 1992 to 2022 for the TRENDY v12 models is {SOC_stock_change:.2f} PgC yr-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIF_GPP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
